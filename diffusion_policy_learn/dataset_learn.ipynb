{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**此为官方实现的dataset代码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from diffusion_policy.model.common.normalizer import LinearNormalizer\n",
    "\n",
    "class BaseLowdimDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    用于低维数据集的数据集结构\n",
    "    \"\"\"\n",
    "    def get_validation_dataset(self) -> 'BaseLowdimDataset':\n",
    "        # return an empty dataset by default\n",
    "        return BaseLowdimDataset()\n",
    "\n",
    "    def get_normalizer(self, **kwargs) -> LinearNormalizer:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_all_actions(self) -> torch.Tensor:\n",
    "        #获取动作信息\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    #dataset必须实现的两个方法\n",
    "    def __len__(self) -> int:\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        output:\n",
    "            obs: T, Do\n",
    "            action: T, Da\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "#用于定义基于图像特征的数据集结构\n",
    "class BaseImageDataset(torch.utils.data.Dataset):\n",
    "    def get_validation_dataset(self) -> 'BaseLowdimDataset':\n",
    "        # return an empty dataset by default\n",
    "        return BaseImageDataset()\n",
    "\n",
    "    def get_normalizer(self, **kwargs) -> LinearNormalizer:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_all_actions(self) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        output:\n",
    "            obs: \n",
    "                key: T, *\n",
    "            action: T, Da\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#是递归地对字典中所有的张量应用指定的函数 func，并保持字典的嵌套结构。\n",
    "def dict_apply(\n",
    "        x: Dict[str, torch.Tensor], \n",
    "        func: Callable[[torch.Tensor], torch.Tensor]\n",
    "        ) -> Dict[str, torch.Tensor]:\n",
    "    result = dict()\n",
    "    for key, value in x.items():\n",
    "        if isinstance(value, dict):\n",
    "            result[key] = dict_apply(value, func)\n",
    "        else:\n",
    "            result[key] = func(value)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# @numba.jit(nopython=True)\n",
    "def create_indices(\n",
    "    episode_ends:np.ndarray, sequence_length:int, \n",
    "    episode_mask: np.ndarray,#表示哪些episode是有效的，需要处理的\n",
    "    pad_before: int=0, pad_after: int=0,\n",
    "    debug:bool=True) -> np.ndarray:\n",
    "    episode_mask.shape == episode_ends.shape        \n",
    "    pad_before = min(max(pad_before, 0), sequence_length-1)#\n",
    "    pad_after = min(max(pad_after, 0), sequence_length-1)\n",
    "\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        if not episode_mask[i]:\n",
    "            # skip episode\n",
    "            continue\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]#开始的索引（全局）\n",
    "        end_idx = episode_ends[i]#结束的索引（全局）\n",
    "        episode_length = end_idx - start_idx#该episode的长度\n",
    "        \n",
    "        min_start = -pad_before#最小开始\n",
    "        max_start = episode_length - sequence_length + pad_after#最大开始\n",
    "        \n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx#全局\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx#全局\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)#local\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx#loacl\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            if debug:\n",
    "                assert(start_offset >= 0)\n",
    "                assert(end_offset >= 0)\n",
    "                assert (sample_end_idx - sample_start_idx) == (buffer_end_idx - buffer_start_idx)\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx, \n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceSampler:\n",
    "    def __init__(self, \n",
    "        replay_buffer: ReplayBuffer, \n",
    "        sequence_length:int,\n",
    "        pad_before:int=0,\n",
    "        pad_after:int=0,\n",
    "        keys=None,\n",
    "        key_first_k=dict(),\n",
    "        episode_mask: Optional[np.ndarray]=None,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        key_first_k: dict str: int\n",
    "            Only take first k data from these keys (to improve perf)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        assert(sequence_length >= 1)\n",
    "        if keys is None:\n",
    "            keys = list(replay_buffer.keys())\n",
    "        \n",
    "        episode_ends = replay_buffer.episode_ends[:]\n",
    "        if episode_mask is None:\n",
    "            episode_mask = np.ones(episode_ends.shape, dtype=bool)\n",
    "\n",
    "        if np.any(episode_mask):\n",
    "            indices = create_indices(episode_ends, \n",
    "                sequence_length=sequence_length, \n",
    "                pad_before=pad_before, \n",
    "                pad_after=pad_after,\n",
    "                episode_mask=episode_mask\n",
    "                )\n",
    "        else:\n",
    "            indices = np.zeros((0,4), dtype=np.int64)\n",
    "\n",
    "        # (buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx)\n",
    "        self.indices = indices \n",
    "        self.keys = list(keys) # prevent OmegaConf list performance problem\n",
    "        self.sequence_length = sequence_length\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.key_first_k = key_first_k\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    #由indices开始采数据  \n",
    "    def sample_sequence(self, idx):\n",
    "        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx \\\n",
    "            = self.indices[idx]\n",
    "        result = dict()\n",
    "        for key in self.keys:\n",
    "            input_arr = self.replay_buffer[key]\n",
    "            # performance optimization, avoid small allocation if possible\n",
    "            if key not in self.key_first_k:\n",
    "                sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "            else:\n",
    "                # performance optimization, only load used obs steps\n",
    "                n_data = buffer_end_idx - buffer_start_idx\n",
    "                k_data = min(self.key_first_k[key], n_data)\n",
    "                # fill value with Nan to catch bugs\n",
    "                # the non-loaded region should never be used\n",
    "                sample = np.full((n_data,) + input_arr.shape[1:], \n",
    "                    fill_value=np.nan, dtype=input_arr.dtype)\n",
    "                try:\n",
    "                    sample[:k_data] = input_arr[buffer_start_idx:buffer_start_idx+k_data]\n",
    "                except Exception as e:\n",
    "                    import pdb; pdb.set_trace()\n",
    "            data = sample\n",
    "            if (sample_start_idx > 0) or (sample_end_idx < self.sequence_length):\n",
    "                data = np.zeros(\n",
    "                    shape=(self.sequence_length,) + input_arr.shape[1:],\n",
    "                    dtype=input_arr.dtype)\n",
    "                if sample_start_idx > 0:\n",
    "                    data[:sample_start_idx] = sample[0]\n",
    "                if sample_end_idx < self.sequence_length:\n",
    "                    data[sample_end_idx:] = sample[-1]\n",
    "                data[sample_start_idx:sample_end_idx] = sample\n",
    "            result[key] = data\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PushTLowdimDataset(BaseLowdimDataset):\n",
    "    def __init__(self, \n",
    "            zarr_path, \n",
    "            horizon=1,\n",
    "            pad_before=0,\n",
    "            pad_after=0,\n",
    "            obs_key='keypoint',\n",
    "            state_key='state',\n",
    "            action_key='action',\n",
    "            seed=42,\n",
    "            val_ratio=0.0,\n",
    "            max_train_episodes=None\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.replay_buffer = ReplayBuffer.copy_from_path(\n",
    "            zarr_path, keys=[obs_key, state_key, action_key])\n",
    "\n",
    "        val_mask = get_val_mask(\n",
    "            n_episodes=self.replay_buffer.n_episodes, \n",
    "            val_ratio=val_ratio,\n",
    "            seed=seed)\n",
    "        train_mask = ~val_mask\n",
    "        train_mask = downsample_mask(\n",
    "            mask=train_mask, \n",
    "            max_n=max_train_episodes, \n",
    "            seed=seed)\n",
    "\n",
    "        self.sampler = SequenceSampler(\n",
    "            replay_buffer=self.replay_buffer, \n",
    "            sequence_length=horizon,\n",
    "            pad_before=pad_before, \n",
    "            pad_after=pad_after,\n",
    "            episode_mask=train_mask\n",
    "            )\n",
    "        self.obs_key = obs_key\n",
    "        self.state_key = state_key\n",
    "        self.action_key = action_key\n",
    "        self.train_mask = train_mask\n",
    "        self.horizon = horizon\n",
    "        self.pad_before = pad_before\n",
    "        self.pad_after = pad_after\n",
    "\n",
    "    def get_validation_dataset(self):\n",
    "        val_set = copy.copy(self)\n",
    "        val_set.sampler = SequenceSampler(\n",
    "            replay_buffer=self.replay_buffer, \n",
    "            sequence_length=self.horizon,\n",
    "            pad_before=self.pad_before, \n",
    "            pad_after=self.pad_after,\n",
    "            episode_mask=~self.train_mask\n",
    "            )\n",
    "        val_set.train_mask = ~self.train_mask\n",
    "        return val_set\n",
    "\n",
    "    def get_normalizer(self, mode='limits', **kwargs):\n",
    "        data = self._sample_to_data(self.replay_buffer)\n",
    "        normalizer = LinearNormalizer()\n",
    "        normalizer.fit(data=data, last_n_dims=1, mode=mode, **kwargs)\n",
    "        return normalizer\n",
    "\n",
    "    def get_all_actions(self) -> torch.Tensor:\n",
    "        return torch.from_numpy(self.replay_buffer[self.action_key])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sampler)\n",
    "\n",
    "    def _sample_to_data(self, sample):\n",
    "        keypoint = sample[self.obs_key]\n",
    "        state = sample[self.state_key]\n",
    "        agent_pos = state[:,:2]\n",
    "        obs = np.concatenate([\n",
    "            keypoint.reshape(keypoint.shape[0], -1), \n",
    "            agent_pos], axis=-1)\n",
    "\n",
    "        data = {\n",
    "            'obs': obs, # T, D_o\n",
    "            'action': sample[self.action_key], # T, D_a\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.sampler.sample_sequence(idx)\n",
    "        data = self._sample_to_data(sample)\n",
    "\n",
    "        torch_data = dict_apply(data, torch.from_numpy)\n",
    "        return torch_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
